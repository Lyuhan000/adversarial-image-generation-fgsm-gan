# adversarial-image-generation-fgsm-gan
Adversarial image generation using Fast Gradient Sign Method (FGSM) and GANs, exploring attack effectiveness, visual realism, and defense evasion.

# Adversarial Image Generation with FGSM on CLIP using CIFAR-100

This repository implements the **Fast Gradient Sign Method (FGSM)** to generate adversarial perturbations targeting the OpenAI CLIP model, with evaluation conducted on the CIFAR-100 benchmark. The work forms part of a broader study into adversarial robustness in multimodal vision–language architectures, focusing on the susceptibility of joint image–text embedding spaces to gradient-based attacks.

## Overview

The project investigates how carefully constrained, gradient-aligned perturbations can induce significant changes in model predictions while preserving high perceptual similarity to the original images. Using CIFAR-100 as a benchmark, FGSM is applied to perturb inputs in the direction of the loss gradient with respect to the model’s top-ranked prediction. This setup enables both quantitative and qualitative analysis of the vulnerabilities in CLIP’s visual encoder, as well as the potential for cross-modal transfer of adversarial features in downstream tasks.

## Methodology

The approach follows a three-stage process:

- **Model and Dataset**  
  The experiments use the pre-trained `ViT-B/32` CLIP model, with CIFAR-100 images paired to textual class descriptions for similarity scoring.

- **Adversarial Perturbation via FGSM**  
  Perturbations are generated by adding a scaled sign of the gradient of the classification loss with respect to the input image. The perturbation magnitude is bounded by an ε-constraint to maintain imperceptibility.

- **Evaluation Protocol**  
  The pipeline compares CLIP’s top-5 predictions before and after perturbation, measuring classification shifts and visualising original–adversarial image pairs.


## Features
- End-to-end PyTorch implementation of FGSM for CLIP.  
- Fully reproducible with CIFAR-100 test images.  
- Adjustable `epsilon` parameter for controlling perturbation strength.  
- Visualisation of original vs. adversarial results for qualitative analysis.

## Example Output
An example run shows that a correctly classified CIFAR-100 image can be misclassified into an unrelated category after applying minimal FGSM perturbation, with confidence scores significantly shifted.

## Requirements
- Python 3.8+
- PyTorch
- torchvision
- OpenAI CLIP
- matplotlib

## Usage
1. Open the notebook `FGSM8_4.ipynb` in Google Colab or Jupyter Notebook.  
2. If running in Colab, make sure to set the runtime to **GPU** for faster execution.  
3. Run all cells sequentially to:
   - Load the CLIP model and CIFAR-100 dataset
   - Generate adversarial images using FGSM
   - Compare original vs. perturbed predictions and visualisations

